{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCR6PDZSqs1y"
      },
      "source": [
        "# Environment setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAOZIX9npx5L"
      },
      "source": [
        "## Downloading the code and the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJyXAaJwrZSt"
      },
      "source": [
        "First off, we'll be installing the required dependencies with `pip`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWV22Epo_fzz"
      },
      "source": [
        "!pip --quiet install imagecodecs tifffile stardist gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7czt0cTbr1LM"
      },
      "source": [
        "Then we obtain the code from the GitHub repository and `cd` into its directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LBe8ZWupwjK"
      },
      "source": [
        "!git clone 'https://github.com/CRefice/ml-segmentation-project.git' --quiet\n",
        "%cd ml-segmentation-project/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dIwnVEGsLIJ"
      },
      "source": [
        "The GitHub repository features a handy script to download the dataset, so we just run it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T78y96E5qF85"
      },
      "source": [
        "!./fetch-data.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P9kMsieqnhI"
      },
      "source": [
        "## Useful imports and settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARQduCiPs_ef"
      },
      "source": [
        "Before running any code, we import all the required packages, functions and modules. We also initialize some variables such as the default device to use while training (GPU if available) and the random seed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAnrppYwpg0S"
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.utils.data as data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage import measure\n",
        "\n",
        "from stardist import fill_label_holes, random_label_cmap, gputools_available\n",
        "from tqdm import tqdm\n",
        "from stardist.matching import matching_dataset\n",
        "from stardist.models import StarDist2D\n",
        "from stardist.models import Config2D, StarDist2D, StarDistData2D\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import unet\n",
        "from dataset import *\n",
        "from train import train_model\n",
        "from losses import *\n",
        "from losses import CombinedLoss, DiceLoss\n",
        "from watershed import label_with_watershed\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3WiQd_0sDV4"
      },
      "source": [
        "# Defining the dataset and data transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ro5ektxs8sq"
      },
      "source": [
        "We have previously downloaded the dataset. Now we define the objects that will load it into memory and perform the required transformations. That includes augmentations, which will greatly increase the size of the dataset (and thus the training time.) To avoid that, leave the `AUGMENT_DATASET` checkbox unchecked.\n",
        "\n",
        "The other parameter to edit is `NUM_CLASSES`, which dictates whether the U-Net will be trained for binary (background-foreground) classification or three-class (with border) segmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIAq0erRreZ-"
      },
      "source": [
        "AUGMENT_DATASET = False #@param {type:\"boolean\"}\n",
        "\n",
        "IMG_SIZE = 1024\n",
        "NUM_CLASSES = 3 #@param {type:\"slider\", min:2, max:3, step:1}\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    NormalizeMinMax(),  \n",
        "    transforms.ToTensor(),\n",
        "    PadToSize(IMG_SIZE),\n",
        "])\n",
        "\n",
        "cell_instance_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    PadToSize(IMG_SIZE),\n",
        "])\n",
        "\n",
        "cell_instance_dataset = CellSegmentationDataset(raw_img_dir=Path(\"dataset/rawimages\"),\n",
        "                                  ground_truth_dir=Path(\"dataset/groundtruth\"),\n",
        "                                  transform=image_transform,\n",
        "                                  target_transform=cell_instance_transform\n",
        "                                )\n",
        "\n",
        "target_transform = transforms.Compose([\n",
        "    InstanceToTwoClass() if NUM_CLASSES == 2 else InstanceToThreeClass(),\n",
        "    transforms.ToTensor(),\n",
        "    PadToSize(IMG_SIZE),\n",
        "    transforms.ConvertImageDtype(torch.float32 if NUM_CLASSES == 2 else torch.long),\n",
        "    # We need to add a dimension to 2-class case since loss functions\n",
        "    # expect a \"channel\" dimension whereas 3-class loss functions don't\n",
        "    transforms.Lambda(lambda img: img.squeeze() if NUM_CLASSES == 3 else img),\n",
        "])\n",
        "\n",
        "if not AUGMENT_DATASET:\n",
        "    dataset = CellSegmentationDataset(raw_img_dir=Path(\"dataset/rawimages\"),\n",
        "                                  ground_truth_dir=Path(\"dataset/groundtruth\"),\n",
        "                                  transform=image_transform,\n",
        "                                  target_transform=target_transform\n",
        "                                )\n",
        "else:\n",
        "    def rotation(degrees):\n",
        "        return lambda img: TF.rotate(img, degrees)\n",
        "\n",
        "    augmentations = [\n",
        "        TF.hflip,\n",
        "        TF.vflip,\n",
        "        lambda img: TF.hflip(TF.vflip(img)),\n",
        "        rotation(90),\n",
        "        rotation(180),\n",
        "        rotation(270),\n",
        "    ]\n",
        "    dataset = AugmentedCellSegmentationDataset(raw_img_dir=Path(\"dataset/rawimages\"),\n",
        "                                  ground_truth_dir=Path(\"dataset/groundtruth\"),\n",
        "                                  augmentations=augmentations,\n",
        "                                  transform=image_transform,\n",
        "                                  target_transform=target_transform\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OfRfXYtzNlC"
      },
      "source": [
        "As a final step, we split the dataset into a train set and a validation set, before creating `DataLoader` objects that will schedule the loading of the images into batches. The ratio of train data out of the entire dataset can be controlled with the slider below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTlpIBhkzHLm"
      },
      "source": [
        "TRAIN_PERCENT = 0.8 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "def split_train_val(dataset):\n",
        "    # For reproducibility\n",
        "    np.random.seed(42)\n",
        "\n",
        "    train_size = int(len(dataset) * TRAIN_PERCENT)\n",
        "    test_size = len(dataset) - train_size\n",
        "    \n",
        "    idx = np.random.permutation(len(dataset))\n",
        "    train_idx = idx[:train_size]\n",
        "    val_idx = idx[:test_size]\n",
        "    \n",
        "    train_dataset = data.Subset(dataset, train_idx)\n",
        "    val_dataset = data.Subset(dataset, val_idx)\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "train_dataset, val_dataset = split_train_val(dataset)\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_loader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6d9eebEuWdh"
      },
      "source": [
        "# U-Net Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2CNTdD8zQSA"
      },
      "source": [
        "Let's start by defining the U-Net model we'll be using to segment images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p04AOeCPpg0z"
      },
      "source": [
        "model = unet.UNet(num_classes=NUM_CLASSES).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U901cMWxzfni"
      },
      "source": [
        "Now, we could train the model \"live\" directly on the dataset, or we could download a model we previously trained on the same data. Choose one of the following options, depending on the amount of time you have ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vssg3JEWzzc4"
      },
      "source": [
        "## Option 1: Training the U-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFo7evJPz5wc"
      },
      "source": [
        "Try editing the parameters in the following cell to affect the training process (especially TRN_EPOCHS and LEARNING_RATE)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBHZ9bjBpg1C"
      },
      "source": [
        "TRN_EPOCHS = 20\n",
        "WEIGHT_DECAY = 0\n",
        "LEARNING_RATE = 0.001\n",
        "MOMENTUM = 0.99\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "if NUM_CLASSES == 2:\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "else:\n",
        "    labels_dataset = (label for image, label in train_loader)\n",
        "    weights = find_class_weights(NUM_CLASSES, labels_dataset).to(device)\n",
        "    criterion = CombinedLoss(\n",
        "        nn.CrossEntropyLoss(weights),\n",
        "        DiceLoss()\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPYWdTII-Xr-"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "model, _ = train_model(device, model, optimizer, criterion, train_loader, val_loader, num_epochs=TRN_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F4P3sy2PBAR"
      },
      "source": [
        "## Option 2: Loading a pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gc9vRu01V_c"
      },
      "source": [
        "Run the following cell to download the model from our Google Drive (should be really quick). The model downloaded also depends on `NUM_CLASSES` as set previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-unmz0PgPOUO"
      },
      "source": [
        "import gdown\n",
        "\n",
        "URL_2_CLASS = \"https://drive.google.com/uc?id=1yjGfzptfhdwm8f6Jlzv8EnmEXP2fAUwL\"\n",
        "URL_3_CLASS = \"https://drive.google.com/uc?id=1zTuKtWULJYxamlxAnp1iZQiOEJipwrr9\"\n",
        "\n",
        "URL = URL_2_CLASS if NUM_CLASSES == 2 else URL_3_CLASS\n",
        "gdown.download(URL_3_CLASS, \"trained-unet.model\", quiet=False)\n",
        "model.load_state_dict(torch.load(\"trained-unet.model\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_dZI3VDqFp8"
      },
      "source": [
        "## Generating predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InpcsFBerI0t"
      },
      "source": [
        "We now generate predictions for one batch of validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-ujgopgrNRK"
      },
      "source": [
        "inputs, labels = next(iter(val_loader))\n",
        "outputs = model.predict(inputs.to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRhcYtim6JNf"
      },
      "source": [
        "We then compare the generated predictions with the ground truth data by displaying them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DljbFU_qPWM"
      },
      "source": [
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "def show_results(inputs, labels, outputs):\n",
        "    \"\"\" Display batches of three images side by side \"\"\"\n",
        "    count = len(inputs)\n",
        "    fig = plt.figure(figsize=(6 * 5, 6 * count))\n",
        "    axs = fig.subplots(count, 3)\n",
        "    \n",
        "    for ax in axs.flat:\n",
        "        ax.axis(\"off\")\n",
        "    \n",
        "    column_labels = [\"Input image\", \"Ground truth\", \"Generated prediction\"]\n",
        "\n",
        "    if count == 1:\n",
        "        for ax, label in zip(axs, column_labels):\n",
        "            ax.set_title(label)\n",
        "        \n",
        "        for i in range(count):\n",
        "            axs[0].imshow(inputs[i].squeeze(), cmap=\"gray\")\n",
        "            axs[1].imshow(labels[i].squeeze(), cmap=\"viridis\")\n",
        "            pred = axs[2].imshow(outputs[i].squeeze(), cmap=\"viridis\")\n",
        "            plt.colorbar(pred, label=\"Prediction confidence\")\n",
        "    else:\n",
        "        for ax, label in zip(axs[0], column_labels):\n",
        "            ax.set_title(label)\n",
        "        \n",
        "        for i in range(count):\n",
        "            axs[i, 0].imshow(inputs[i].squeeze(), cmap=\"gray\")\n",
        "            axs[i, 1].imshow(labels[i].squeeze(), cmap=\"viridis\")\n",
        "            pred = axs[i, 2].imshow(outputs[i].squeeze(), cmap=\"viridis\")\n",
        "    plt.colorbar(pred, ax=axs.ravel().tolist(), label=\"Prediction confidence\")\n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNx03DMq6TqK"
      },
      "source": [
        "You can edit the `class_index` slider to see what the model predicted for each of the different classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksX_vx4eVVxh"
      },
      "source": [
        "class_index = 1 #@param {type:\"slider\", min:0, max:2, step:1}\n",
        "class_index = min(class_index, NUM_CLASSES - 1)\n",
        "\n",
        "class_outputs = outputs == class_index if NUM_CLASSES == 2 else outputs[:, class_index]\n",
        "\n",
        "show_results(inputs, labels == class_index, class_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-un_FaoI7zRE"
      },
      "source": [
        "We can use this data with either a connected components or a watershed algorithm to obtain an instance segmentation of an image. We'll see later how to do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVSUrLhgzI1N"
      },
      "source": [
        "# Stardist Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJcPspcE7F9M"
      },
      "source": [
        "Once again, we can either train the model ourselves or download a pre-trained one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZWaA_xIHMzj"
      },
      "source": [
        "## Option 1.Stardist Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoR2ocRONS5z"
      },
      "source": [
        "We start by defining the parameters for the model we'll be using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufVYnMT4st9R"
      },
      "source": [
        "# 32 is a good default choice\r\n",
        "n_rays = 32\r\n",
        "\r\n",
        "# Use OpenCL-based computations for data generator during training (requires 'gputools')\r\n",
        "use_gpu = gputools_available()\r\n",
        "\r\n",
        "# Predict on subsampled grid for increased efficiency and larger field of view\r\n",
        "grid = (4,4)\r\n",
        "\r\n",
        "conf = Config2D (\r\n",
        "    n_rays       = n_rays,\r\n",
        "    grid         = grid,\r\n",
        "    use_gpu      = use_gpu,\r\n",
        "    n_channel_in = 1,\r\n",
        ")\r\n",
        "\r\n",
        "if use_gpu:\r\n",
        "    from csbdeep.utils.tf import limit_gpu_memory\r\n",
        "    # adjust as necessary: limit GPU memory to be used by TensorFlow to leave some to OpenCL-based computations\r\n",
        "    limit_gpu_memory(0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5hYSjxFNefN"
      },
      "source": [
        "We proceed by instantiating the StarDist model with the specified configuration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG_PwkzHuEPZ"
      },
      "source": [
        "stardist_model = StarDist2D(conf, name='stardist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MKdpJG3Nme5"
      },
      "source": [
        "Stardist wants input data in a slightly different format than our U-Net, so we write a conversion function to do the job for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDs-8lDtKQB5"
      },
      "source": [
        "def convert_dataset(dataset):\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    for input, label in dataset:\n",
        "        inputs.append(input.numpy().squeeze())\n",
        "        labels.append(fill_label_holes(label.numpy().squeeze()))\n",
        "    return (inputs, labels)\n",
        "\n",
        "train_cell_dataset, val_cell_dataset = split_train_val(cell_instance_dataset)\n",
        "inputs_train, labels_train = convert_dataset(train_cell_dataset)\n",
        "val_data = convert_dataset(val_cell_dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LusWK4vvNwWS"
      },
      "source": [
        "We also define the augmentations we'll be using on the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3WdCgQkuTiO"
      },
      "source": [
        "def random_fliprot(img, mask): \r\n",
        "    assert img.ndim >= mask.ndim\r\n",
        "    axes = tuple(range(mask.ndim))\r\n",
        "    perm = tuple(np.random.permutation(axes))\r\n",
        "    img = img.transpose(perm + tuple(range(mask.ndim, img.ndim))) \r\n",
        "    mask = mask.transpose(perm) \r\n",
        "    for ax in axes: \r\n",
        "        if np.random.rand() > 0.5:\r\n",
        "            img = np.flip(img, axis=ax)\r\n",
        "            mask = np.flip(mask, axis=ax)\r\n",
        "    return img, mask \r\n",
        "\r\n",
        "def random_intensity_change(img):\r\n",
        "    img = img*np.random.uniform(0.6,2) + np.random.uniform(-0.2,0.2)\r\n",
        "    return img\r\n",
        "\r\n",
        "\r\n",
        "def augmenter(x, y):\r\n",
        "    \"\"\"Augmentation of a single input/label image pair.\r\n",
        "    x is an input image\r\n",
        "    y is the corresponding ground-truth label image\r\n",
        "    \"\"\"\r\n",
        "    x, y = random_fliprot(x, y)\r\n",
        "    x = random_intensity_change(x)\r\n",
        "    # add some gaussian noise\r\n",
        "    sig = 0.02*np.random.uniform(0,1)\r\n",
        "    x = x + sig*np.random.normal(0,1,x.shape)\r\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEFxtqW6N5jG"
      },
      "source": [
        "And finally, we run the training process. You can play around with the `NUM_EPOCHS` and `STEPS_PER_EPOCH` parameters to see how they influence the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rmbkF9Muto2"
      },
      "source": [
        "NUM_EPOCHS = 30\r\n",
        "STEPS_PER_EPOCH = 50\r\n",
        "\r\n",
        "stardist_model.train(inputs_train, labels_train, validation_data=val_data, augmenter=augmenter, epochs=NUM_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_PaE59JxpZR"
      },
      "source": [
        "stardist_model.optimize_thresholds(*val_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBjJYp7lH9M7"
      },
      "source": [
        "## Option 2. Downloading a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "579cZ82sdBMp"
      },
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1Xp5lSCOBHc_JX1nnupX8vhzI8CgfXZiu\" -O stardist-pretrained.zip\n",
        "!unzip stardist-pretrained.zip -dstardist-pretrained"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-o1RrytzV_C"
      },
      "source": [
        "stardist_model = StarDist2D(None, name='stardist-pretrained')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utTEEr7Y7XaR"
      },
      "source": [
        "# Comparing segmentation results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOIsWeVR7ik6"
      },
      "source": [
        "We can now obtain instance segmentations through different methods:\n",
        "\n",
        "1. U-Net with connected components\n",
        "2. U-Net with watershed\n",
        "3. Stardist\n",
        "\n",
        "As a final step, we visualize them and compare their results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwoPoy9q7cmo"
      },
      "source": [
        "## Visualizing segmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1h48Tdw8kRN"
      },
      "source": [
        "We generate the instance segmentations for a batch of data with the different methods and then plot them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDPXpvIg80s7"
      },
      "source": [
        "inputs, _ = next(iter(val_loader))\n",
        "outputs = model.predict(inputs.to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB8VzadyaAXZ"
      },
      "source": [
        "def show_segmentation_results(input, connected, watershed, stardist, cmap):\n",
        "    fig = plt.figure(figsize=(13, 13))\n",
        "    axs = fig.subplots(2, 2).flat\n",
        "    \n",
        "    column_labels = [\"Input image\", \"Connected Components\", \"Watershed\", \"Stardist\"]\n",
        "\n",
        "    for ax, label in zip(axs, column_labels):\n",
        "        ax.axis(\"off\")\n",
        "        ax.set_title(label)\n",
        "        \n",
        "    axs[0].imshow(input.squeeze(), cmap=\"gray\")\n",
        "    axs[1].imshow(connected, interpolation=\"nearest\", cmap=cmap)\n",
        "    axs[2].imshow(watershed, interpolation=\"nearest\", cmap=cmap)\n",
        "    axs[3].imshow(stardist, interpolation=\"nearest\", cmap=cmap)\n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcFfrFwIbOED"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "for input, output in zip(inputs, outputs):\n",
        "    input = input.numpy()\n",
        "    output_classes = output.argmax(dim=0).numpy()\n",
        "    watershed_output = label_with_watershed(output_classes == 1)\n",
        "    connected_output = measure.label(output_classes == 1)\n",
        "    stardist_output, _ = stardist_model.predict_instances(input.squeeze())\n",
        "    \n",
        "    show_segmentation_results(\n",
        "        input,\n",
        "        connected_output,\n",
        "        watershed_output,\n",
        "        stardist_output,\n",
        "        cmap=random_label_cmap()\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M-S2YCdBh7k"
      },
      "source": [
        "## Visualizing the accuracy of the methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr88fazp96qH"
      },
      "source": [
        "Finally, we compute the accuracy of our methods using the IoU metric and plot them as a function of the IoU threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RchypDAZYz7j"
      },
      "source": [
        "label_list = []\r\n",
        "water_list = []\r\n",
        "connec_list = []\r\n",
        "star_list = []\r\n",
        "UNet_list = []\r\n",
        "\r\n",
        "for inputs, labels in val_loader:\r\n",
        "  outputs = model.predict(inputs.to(device)).argmax(dim=1).numpy()\r\n",
        "  for i in range(BATCH_SIZE):\r\n",
        "    label_list.append(labels[i].numpy().squeeze())\r\n",
        "    connec_list.append(measure.label(outputs[i]))\r\n",
        "    UNet_list.append(label_with_watershed(outputs[i]))\r\n",
        "    pred_star, details = stardist_model.predict_instances(inputs[i].numpy().squeeze())\r\n",
        "    star_list.append(pred_star)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG8uWvmcMhbP"
      },
      "source": [
        "taus = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\r\n",
        "stats = [matching_dataset(connec_list, star_list, thresh=t, show_progress=False) for t in tqdm(taus)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHTjV-e6O5bU"
      },
      "source": [
        "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(15,5))\r\n",
        "\r\n",
        "for m in ('precision', 'recall', 'accuracy', 'f1', 'mean_true_score', 'mean_matched_score', 'panoptic_quality'):\r\n",
        "    ax1.plot(taus, [s._asdict()[m] for s in stats], '.-', lw=2, label=m)\r\n",
        "ax1.set_xlabel(r'IoU threshold $\\tau$')\r\n",
        "ax1.set_ylabel('Metric value')\r\n",
        "ax1.grid()\r\n",
        "ax1.legend()\r\n",
        "\r\n",
        "for m in ('fp', 'tp', 'fn'):\r\n",
        "    ax2.plot(taus, [s._asdict()[m] for s in stats], '.-', lw=2, label=m)\r\n",
        "ax2.set_xlabel(r'IoU threshold $\\tau$')\r\n",
        "ax2.set_ylabel('Number #')\r\n",
        "ax2.grid()\r\n",
        "ax2.legend();\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}