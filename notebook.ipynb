{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "import unet\n",
    "import pytorch_data_generator\n",
    "import data_loading\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "TRN_EPOCHS = 10\n",
    "TRAIN_SIZE = 2000\n",
    "\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, all_labels = load_dataset(\"Ganglioneuroblastoma\", threshold=True, Path = Path(\"dataset\"))\n",
    "train_data, train_labels = all_data[:TRAIN_SIZE, :, :, :], all_labels[:TRAIN_SIZE, :, :, :]\n",
    "val_data, val_labels = all_data[TRAIN_SIZE:, :, :, :], all_labels[TRAIN_SIZE:, :, :, :]\n",
    "\n",
    "train_set = DataGenerator(train_data, train_labels)\n",
    "val_set = DataGenerator(val_data, val_labels)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = unet.UNet()\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size=(1, 1000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 10e3\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            epoch_loss = []\n",
    "            \n",
    "            if phase == 'train':\n",
    "#                 scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)             \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                                      \n",
    "                epoch_loss.append(loss)\n",
    "            \n",
    "            print(\"Epoch {} loss: {}\".format(phase, np.mean(epoch_loss)))\n",
    "                \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "model = unet.UNet()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = LEARNING_RATE, momentum=MOMENTUM)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Maybe interesting to use later\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
    "\n",
    "model = train_model(model, optimizer_ft, criterion, exp_lr_scheduler, num_epochs=TRN_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
