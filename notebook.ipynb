{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCR6PDZSqs1y"
      },
      "source": [
        "# Environment setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAOZIX9npx5L"
      },
      "source": [
        "## Download code and data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWV22Epo_fzz",
        "outputId": "bded3e93-6834-42e7-b9e0-6288ccc1c6d0"
      },
      "source": [
        "!pip install imagecodecs tifffile torchgeometry"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting imagecodecs\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/d0/09edd29531c81bb95919a687c14eafaf9867fdefabfdafc27b139434a394/imagecodecs-2020.5.30-cp36-cp36m-manylinux2014_x86_64.whl (17.9MB)\n",
            "\u001b[K     |████████████████████████████████| 17.9MB 210kB/s \n",
            "\u001b[?25hRequirement already satisfied: tifffile in /usr/local/lib/python3.6/dist-packages (2020.9.3)\n",
            "Collecting torchgeometry\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/d6/3f6820c0589bc3876080c59b58a3bad11af746a7b46f364b1cde7972bd72/torchgeometry-0.1.2-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.6/dist-packages (from imagecodecs) (1.18.5)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from torchgeometry) (1.7.0+cu101)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->torchgeometry) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->torchgeometry) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->torchgeometry) (3.7.4.3)\n",
            "Installing collected packages: imagecodecs, torchgeometry\n",
            "Successfully installed imagecodecs-2020.5.30 torchgeometry-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LBe8ZWupwjK",
        "outputId": "ff01da85-518a-46f8-8344-4f8090fc7c53"
      },
      "source": [
        "# Clone the GitHub repository and cd into it\n",
        "!git clone 'https://github.com/CRefice/ml-segmentation-project.git'\n",
        "%cd ml-segmentation-project/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ml-segmentation-project'...\n",
            "remote: Enumerating objects: 141, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "remote: Total 141 (delta 74), reused 71 (delta 29), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (141/141), 1.14 MiB | 11.58 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n",
            "/content/ml-segmentation-project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T78y96E5qF85",
        "outputId": "b5f5e20f-5386-440b-e3b9-ade79967b470"
      },
      "source": [
        "# Download the data using the fetch script\n",
        "!./fetch-data.sh"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 72.8M  100 72.8M    0     0  9528k      0  0:00:07  0:00:07 --:--:-- 16.5M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P9kMsieqnhI"
      },
      "source": [
        "## Useful imports and settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAnrppYwpg0S"
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n",
        "from torchvision import transforms, datasets, models\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage.morphology import binary_dilation\n",
        "import os,sys\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "import torchgeometry as tgm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "\n",
        "import unet\n",
        "from dataset import CellSegmentationDataset\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3WiQd_0sDV4"
      },
      "source": [
        "# Data importing and massaging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ro5ektxs8sq"
      },
      "source": [
        "Now we import the dataset and create train/test splitters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIAq0erRreZ-"
      },
      "source": [
        "BATCH_SIZE = 2\n",
        "TRAIN_PERCENT = 0.8\n",
        "CLASS_NUMBER = 3\n",
        "\n",
        "img_transforms = transforms.Compose([\n",
        "      # Convert to 0-1 float\n",
        "      transforms.Lambda(lambda img: (img.astype(np.float32) - np.min(img)) / (np.max(img) - np.min(img))),  \n",
        "      transforms.ToTensor(),\n",
        "      transforms.CenterCrop(1024),\n",
        "])\n",
        "\n",
        "target_transforms = transforms.Compose([\n",
        "      transforms.Lambda(lambda img: img.astype(np.float32)),\n",
        "      transforms.ToTensor(),\n",
        "      # Limit to 0-1 for foreground-background segmentation\n",
        "      transforms.Lambda(lambda img: img.clamp(max=1.0)),\n",
        "      transforms.CenterCrop(1024),\n",
        "])\n",
        "\n",
        "target_bound_transforms = transforms.Compose([\n",
        "      transforms.Lambda(lambda img: img.astype(np.float32)),\n",
        "      transforms.ToTensor(),\n",
        "      # Limit to 0-1 for foreground-background segmentation\n",
        "      transforms.Lambda(lambda img: img.clamp(max=1.0)),\n",
        "      transforms.Lambda(lambda img: img + binary_dilation(img)),\n",
        "      transforms.CenterCrop(1024),\n",
        "])\n",
        "\n",
        "full_dataset = CellSegmentationDataset(raw_img_dir=Path(\"dataset/rawimages\"),\n",
        "                                  ground_truth_dir=Path(\"dataset/groundtruth\"),\n",
        "                                  pattern=\"Neuroblastoma\",\n",
        "                                  transform=img_transforms,\n",
        "                                  target_transform=target_bound_transforms,\n",
        "                                )\n",
        "\n",
        "train_size = int(len(full_dataset) * TRAIN_PERCENT)\n",
        "test_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = data.random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6d9eebEuWdh"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p04AOeCPpg0z"
      },
      "source": [
        "model = unet.UNet(out_channels=CLASS_NUMBER).to(device)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUkSKef5C1SX",
        "scrolled": true,
        "outputId": "9ebbdabb-5316-44c0-d153-7c77758b220a"
      },
      "source": [
        "summary(model, input_size=(1, 1024, 1024))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1       [-1, 64, 1024, 1024]             640\n",
            "              ReLU-2       [-1, 64, 1024, 1024]               0\n",
            "            Conv2d-3       [-1, 64, 1024, 1024]          36,928\n",
            "              ReLU-4       [-1, 64, 1024, 1024]               0\n",
            "         MaxPool2d-5         [-1, 64, 512, 512]               0\n",
            "            Conv2d-6        [-1, 128, 512, 512]          73,856\n",
            "              ReLU-7        [-1, 128, 512, 512]               0\n",
            "            Conv2d-8        [-1, 128, 512, 512]         147,584\n",
            "              ReLU-9        [-1, 128, 512, 512]               0\n",
            "        MaxPool2d-10        [-1, 128, 256, 256]               0\n",
            "           Conv2d-11        [-1, 256, 256, 256]         295,168\n",
            "             ReLU-12        [-1, 256, 256, 256]               0\n",
            "           Conv2d-13        [-1, 256, 256, 256]         590,080\n",
            "             ReLU-14        [-1, 256, 256, 256]               0\n",
            "        MaxPool2d-15        [-1, 256, 128, 128]               0\n",
            "           Conv2d-16        [-1, 512, 128, 128]       1,180,160\n",
            "             ReLU-17        [-1, 512, 128, 128]               0\n",
            "           Conv2d-18        [-1, 512, 128, 128]       2,359,808\n",
            "             ReLU-19        [-1, 512, 128, 128]               0\n",
            "        MaxPool2d-20          [-1, 512, 64, 64]               0\n",
            "           Conv2d-21         [-1, 1024, 64, 64]       4,719,616\n",
            "             ReLU-22         [-1, 1024, 64, 64]               0\n",
            "           Conv2d-23         [-1, 1024, 64, 64]       9,438,208\n",
            "             ReLU-24         [-1, 1024, 64, 64]               0\n",
            "  ConvTranspose2d-25        [-1, 512, 128, 128]       2,097,664\n",
            "           Conv2d-26        [-1, 512, 128, 128]       4,719,104\n",
            "             ReLU-27        [-1, 512, 128, 128]               0\n",
            "           Conv2d-28        [-1, 512, 128, 128]       2,359,808\n",
            "             ReLU-29        [-1, 512, 128, 128]               0\n",
            "  ConvTranspose2d-30        [-1, 256, 256, 256]         524,544\n",
            "           Conv2d-31        [-1, 256, 256, 256]       1,179,904\n",
            "             ReLU-32        [-1, 256, 256, 256]               0\n",
            "           Conv2d-33        [-1, 256, 256, 256]         590,080\n",
            "             ReLU-34        [-1, 256, 256, 256]               0\n",
            "  ConvTranspose2d-35        [-1, 128, 512, 512]         131,200\n",
            "           Conv2d-36        [-1, 128, 512, 512]         295,040\n",
            "             ReLU-37        [-1, 128, 512, 512]               0\n",
            "           Conv2d-38        [-1, 128, 512, 512]         147,584\n",
            "             ReLU-39        [-1, 128, 512, 512]               0\n",
            "  ConvTranspose2d-40       [-1, 64, 1024, 1024]          32,832\n",
            "           Conv2d-41       [-1, 64, 1024, 1024]          73,792\n",
            "             ReLU-42       [-1, 64, 1024, 1024]               0\n",
            "           Conv2d-43       [-1, 64, 1024, 1024]          36,928\n",
            "             ReLU-44       [-1, 64, 1024, 1024]               0\n",
            "           Conv2d-45        [-1, 3, 1024, 1024]             195\n",
            "================================================================\n",
            "Total params: 31,030,723\n",
            "Trainable params: 31,030,723\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 4.00\n",
            "Forward/backward pass size (MB): 9032.00\n",
            "Params size (MB): 118.37\n",
            "Estimated Total Size (MB): 9154.37\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aNhpfoDlHgO"
      },
      "source": [
        "We first define the evaluation functions we'll be using during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6hlfXxLyUsR"
      },
      "source": [
        "def combined_loss(output, label, criterion, bce_weight=.5):\n",
        "  CE_loss = criterion(output, label)\n",
        "  output = F.softmax(output, dim=1)\n",
        "  dice_l = tgm.losses.dice_loss(output, label)\n",
        "  # dice = dice_loss(output, label)\n",
        "  comb_loss = dice_l * bce_weight + CE_loss * (1 - bce_weight)\n",
        "  return comb_loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VpcB2hFlNBT"
      },
      "source": [
        "def evaluate(model, loader):\n",
        "    n_val = len(loader)\n",
        "    tot = 0\n",
        "    for image, labels in loader:\n",
        "        with torch.no_grad():\n",
        "            pred = model(image.to(device))\n",
        "            tot += dice_loss(pred, labels.to(device))\n",
        "    return tot / n_val"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqXvYAoopg08"
      },
      "source": [
        "def train_model(model, optimizer, criterion, scheduler, num_epochs=25):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        since = time.time()\n",
        "\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "        # Training phase\n",
        "        model.train()\n",
        "\n",
        "        epoch_loss = 0\n",
        "        epoch_samples = len(train_loader)\n",
        "            \n",
        "        torch.cuda.empty_cache()\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).long()\n",
        "            labels = labels.squeeze(1)             \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()   \n",
        "            outputs = model(inputs)\n",
        "            loss = combined_loss(outputs, labels, criterion)\n",
        "            loss.backward()\n",
        "            optimizer.step()    \n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        #scheduler.step()\n",
        "        \n",
        "        print(\"Epoch train loss: {}\".format(epoch_loss / epoch_samples))\n",
        "        # print(\"Epoch dice loss: {}\".format(epoch_dice_loss / epoch_samples))\n",
        "\n",
        "        # Evaluation phase\n",
        "        model.eval()\n",
        "        val_loss = evaluate(model, val_loader)\n",
        "        if val_loss < best_loss:\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print(\"Epoch validation loss: {}\".format(val_loss))\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('Took {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBHZ9bjBpg1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2381034e-675e-4b9e-ba04-6c2354cfaa98"
      },
      "source": [
        "TRN_EPOCHS = 20\n",
        "WEIGHT_DECAY = 0\n",
        "LEARNING_RATE = 0.001\n",
        "MOMENTUM = 0.99\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "# Maybe interesting to use later\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
        "\n",
        "model = train_model(model, optimizer, criterion, exp_lr_scheduler, num_epochs=TRN_EPOCHS)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "----------\n",
            "Epoch train loss: 1.4429619397435869\n",
            "Epoch validation loss: 0.6955950856208801\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 2/20\n",
            "----------\n",
            "Epoch train loss: 0.7457970210484096\n",
            "Epoch validation loss: 0.6784295439720154\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 3/20\n",
            "----------\n",
            "Epoch train loss: 0.705968839781625\n",
            "Epoch validation loss: 0.7081351280212402\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 4/20\n",
            "----------\n",
            "Epoch train loss: 0.5889290826661246\n",
            "Epoch validation loss: 0.6811190247535706\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 5/20\n",
            "----------\n",
            "Epoch train loss: 0.420588310275759\n",
            "Epoch validation loss: 0.6071157455444336\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 6/20\n",
            "----------\n",
            "Epoch train loss: 0.3519254518406732\n",
            "Epoch validation loss: 0.6145727038383484\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 7/20\n",
            "----------\n",
            "Epoch train loss: 0.3276971919195993\n",
            "Epoch validation loss: 0.5880736112594604\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 8/20\n",
            "----------\n",
            "Epoch train loss: 0.32460252727781025\n",
            "Epoch validation loss: 0.5966174602508545\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 9/20\n",
            "----------\n",
            "Epoch train loss: 0.3155137130192348\n",
            "Epoch validation loss: 0.5912916660308838\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 10/20\n",
            "----------\n",
            "Epoch train loss: 0.32088529212134226\n",
            "Epoch validation loss: 0.5941126346588135\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 11/20\n",
            "----------\n",
            "Epoch train loss: 0.31308176262038095\n",
            "Epoch validation loss: 0.5881818532943726\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 12/20\n",
            "----------\n",
            "Epoch train loss: 0.3136881802763258\n",
            "Epoch validation loss: 0.5884387493133545\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 13/20\n",
            "----------\n",
            "Epoch train loss: 0.31462218293121885\n",
            "Epoch validation loss: 0.5923556089401245\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 14/20\n",
            "----------\n",
            "Epoch train loss: 0.3130250573158264\n",
            "Epoch validation loss: 0.5905060768127441\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 15/20\n",
            "----------\n",
            "Epoch train loss: 0.3181333158697401\n",
            "Epoch validation loss: 0.5873775482177734\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 16/20\n",
            "----------\n",
            "Epoch train loss: 0.32184710247176035\n",
            "Epoch validation loss: 0.6062643527984619\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 17/20\n",
            "----------\n",
            "Epoch train loss: 0.31129940918513704\n",
            "Epoch validation loss: 0.5870996713638306\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 18/20\n",
            "----------\n",
            "Epoch train loss: 0.31531774571963717\n",
            "Epoch validation loss: 0.5860125422477722\n",
            "Took 0m 14s\n",
            "\n",
            "Epoch 19/20\n",
            "----------\n",
            "Epoch train loss: 0.31162889088903156\n",
            "Epoch validation loss: 0.5967985987663269\n",
            "Took 0m 13s\n",
            "\n",
            "Epoch 20/20\n",
            "----------\n",
            "Epoch train loss: 0.31495297806603567\n",
            "Epoch validation loss: 0.5881903171539307\n",
            "Took 0m 13s\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_dZI3VDqFp8"
      },
      "source": [
        "# Generating predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InpcsFBerI0t"
      },
      "source": [
        "We now generate predictions for one batch of data and compare it to the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-ujgopgrNRK"
      },
      "source": [
        "inputs, labels = next(iter(val_loader))\n",
        "with torch.no_grad():\n",
        "    outputs = model.predict(inputs.to(device))\n",
        "    outputs = F.softmax(outputs, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DljbFU_qPWM"
      },
      "source": [
        "plt.gray()\n",
        "count = len(inputs)\n",
        "fig = plt.figure(figsize=(6 * 3, 6 * count))\n",
        "axs = fig.subplots(count, 3)\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "column_labels = [\"Input image\", \"Ground truth\", \"Generated prediction\"]\n",
        "for ax, label in zip(axs[0], column_labels):\n",
        "    ax.set_title(label)\n",
        "\n",
        "for i in range(count):\n",
        "    axs[i, 0].imshow(inputs[i].cpu().squeeze())\n",
        "    axs[i, 1].imshow(labels[i].cpu().squeeze())\n",
        "    axs[i, 2].imshow(outputs[i].cpu().squeeze())\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu0ZgZBETxwW"
      },
      "source": [
        "image = labels[0,:,:,:].numpy().squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_87EndJiZRlj"
      },
      "source": [
        "np.unique(image + scipy.ndimage.morphology.binary_dilation(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rttfR0jZVtVk"
      },
      "source": [
        "final = image + scipy.ndimage.morphology.binary_dilation(image)\n",
        "final_bound = final\n",
        "final_bound[final_bound==2] = 0\n",
        "final_no_bound = final\n",
        "final_no_bound[final_no_bound==1] = 0\n",
        "final_no_bound[final_no_bound==2] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuXfF_HpRIoR"
      },
      "source": [
        "edges = image - scipy.ndimage.morphology.binary_dilation(image)\n",
        "edges[edges==-1] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFCyjKFDUTkS"
      },
      "source": [
        "dilation = scipy.ndimage.morphology.binary_dilation(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdhg3Ah9UuOZ"
      },
      "source": [
        "plt.gray()\n",
        "count = len(inputs)\n",
        "fig = plt.figure(figsize=(6 * 3, 6 * count))\n",
        "axs = fig.subplots(count, 3)\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "column_labels = [\"Input image\", \"Ground truth\", \"Generated prediction\"]\n",
        "for ax, label in zip(axs[0], column_labels):\n",
        "    ax.set_title(label)\n",
        "\n",
        "for i in range(count):\n",
        "    axs[i, 0].imshow(final)\n",
        "    axs[i, 1].imshow(final_no_bound)\n",
        "    axs[i, 2].imshow(final_bound)\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca29LJEOJTsh"
      },
      "source": [
        "from sklearn.metrics import jaccard_similarity_score as jsc\n",
        "print(jsc(target,lbl))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}